2022-12-16 01:28:23,831 [DEBUG] use config:config.json
2022-12-16 01:28:23,831 [DEBUG] Creating, training and testing models with the following parameters:
2022-12-16 01:28:23,831 [DEBUG] VTI AGG DBC ^VIX
from 2006-01-01 to 2022-10-01 with 1d interval
retrain every 2 year with 50 days

2022-12-16 01:28:23,831 [DEBUG] Getting data
2022-12-16 01:28:23,831 [DEBUG] Fetch data using the following parameters:
2022-12-16 01:28:23,831 [DEBUG] Tickers: VTI AGG DBC ^VIX
2022-12-16 01:28:23,831 [DEBUG] Start date: 2006-01-01
2022-12-16 01:28:23,831 [DEBUG] End date: 2022-10-01
2022-12-16 01:28:23,831 [DEBUG] Interval: 1d
2022-12-16 01:28:23,831 [DEBUG] File dataset_2006-01-01_2022-10-01_VTI_AGG_DBC_^VIX.csv, no need to download, loading file...
2022-12-16 01:28:23,851 [DEBUG]        Unnamed: 0        Date Ticker  ...         Low        Open      Volume
0               0  2006-01-03    AGG  ...   59.801179   59.866725    170600.0
1               1  2006-01-03    VTI  ...   44.781733   45.091552   1769800.0
2               2  2006-01-03   ^VIX  ...   10.990000   12.250000         0.0
3               3  2006-01-04    AGG  ...   59.950149   59.997819    284500.0
4               4  2006-01-04    VTI  ...   45.652858   45.678373    763600.0
...           ...         ...    ...  ...         ...         ...         ...
16836       16836  2022-09-29   ^VIX  ...   31.160000   31.670000         0.0
16837       16837  2022-09-30    AGG  ...   95.320501   95.984443  14376300.0
16838       16838  2022-09-30    DBC  ...   23.840000   24.020000   3947600.0
16839       16839  2022-09-30    VTI  ...  179.279999  181.559998   8431700.0
16840       16840  2022-09-30   ^VIX  ...   29.389999   31.610001         0.0

[16841 rows x 8 columns]
2022-12-16 01:28:23,881 [DEBUG] Training and testing with the device: cuda
2022-12-16 01:28:23,881 [DEBUG] Training model: supertest
2022-12-16 01:28:25,121 [DEBUG] Training from 2006, 2007
2022-12-16 01:28:25,153 [DEBUG] [+] Creating LSTM model
2022-12-16 01:28:25,161 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:28:25,162 [DEBUG] [+] Training model...
[1/3] Train loss : 0.0365, validation loss : 0.0655
[2/3] Train loss : 0.0589, validation loss : 0.0659
[3/3] Train loss : 0.0601, validation loss : 0.0659
2022-12-16 01:29:51,776 [DEBUG] [+] Creating Transformer Encoder model
2022-12-16 01:29:51,796 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:29:51,797 [DEBUG] [+] Training model...
[1/5] Train loss : 0.0449, validation loss : 0.0659
[2/5] Train loss : 0.0602, validation loss : 0.0659
[3/5] Train loss : 0.0601, validation loss : 0.0659
[4/5] Train loss : 0.0601, validation loss : 0.0659
[5/5] Train loss : 0.0601, validation loss : 0.0659
2022-12-16 01:31:59,679 [DEBUG] Training from 2008, 2009
2022-12-16 01:31:59,701 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:31:59,701 [DEBUG] [+] Training model...
[1/3] Train loss : -0.0515, validation loss : 0.0983
[2/3] Train loss : -0.0515, validation loss : 0.0983
[3/3] Train loss : -0.0515, validation loss : 0.0983
2022-12-16 01:34:20,747 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:34:20,749 [DEBUG] [+] Training model...
[1/5] Train loss : -0.0515, validation loss : 0.0984
[2/5] Train loss : -0.0482, validation loss : 0.1034
[3/5] Train loss : -0.0260, validation loss : 0.0380
[4/5] Train loss : -0.0175, validation loss : 0.0380
[5/5] Train loss : -0.0175, validation loss : 0.0380
2022-12-16 01:36:00,899 [DEBUG] Training from 2010, 2011
2022-12-16 01:36:00,922 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:36:00,922 [DEBUG] [+] Training model...
[1/3] Train loss : 0.0423, validation loss : -0.0350
[2/3] Train loss : 0.0423, validation loss : -0.0350
[3/3] Train loss : 0.0423, validation loss : -0.0350
2022-12-16 01:37:55,592 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:37:55,594 [DEBUG] [+] Training model...
[1/5] Train loss : 0.0093, validation loss : -0.0431
[2/5] Train loss : 0.0093, validation loss : -0.0431
[3/5] Train loss : 0.0093, validation loss : -0.0431
[4/5] Train loss : 0.0093, validation loss : -0.0431
[5/5] Train loss : 0.0093, validation loss : -0.0431
2022-12-16 01:39:28,166 [DEBUG] Training from 2012, 2013
2022-12-16 01:39:28,191 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:39:28,192 [DEBUG] [+] Training model...
[1/3] Train loss : 0.0153, validation loss : 0.0429
[2/3] Train loss : 0.0153, validation loss : 0.0429
[3/3] Train loss : 0.0153, validation loss : 0.0429
2022-12-16 01:41:30,337 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:41:30,339 [DEBUG] [+] Training model...
[1/5] Train loss : -0.0141, validation loss : 0.0033
[2/5] Train loss : -0.0141, validation loss : 0.0033
[3/5] Train loss : -0.0141, validation loss : 0.0033
[4/5] Train loss : -0.0141, validation loss : 0.0033
[5/5] Train loss : -0.0141, validation loss : 0.0033
2022-12-16 01:43:07,171 [DEBUG] Training from 2014, 2015
2022-12-16 01:43:07,193 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:43:07,194 [DEBUG] [+] Training model...
[1/3] Train loss : 0.0090, validation loss : -0.0582
[2/3] Train loss : 0.0090, validation loss : -0.0582
[3/3] Train loss : 0.0090, validation loss : -0.0582
2022-12-16 01:45:06,475 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:45:06,477 [DEBUG] [+] Training model...
[1/5] Train loss : -0.0043, validation loss : -0.0604
[2/5] Train loss : -0.0043, validation loss : -0.0604
[3/5] Train loss : -0.0043, validation loss : -0.0604
[4/5] Train loss : -0.0043, validation loss : -0.0604
[5/5] Train loss : -0.0043, validation loss : -0.0604
2022-12-16 01:46:45,255 [DEBUG] Training from 2016, 2017
2022-12-16 01:46:45,284 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:46:45,285 [DEBUG] [+] Training model...
[1/3] Train loss : 0.0106, validation loss : 0.0688
[2/3] Train loss : 0.0106, validation loss : 0.0688
[3/3] Train loss : 0.0106, validation loss : 0.0688
2022-12-16 01:48:50,100 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:48:50,102 [DEBUG] [+] Training model...
[1/5] Train loss : -0.0047, validation loss : 0.0255
[2/5] Train loss : -0.0047, validation loss : 0.0255
[3/5] Train loss : -0.0047, validation loss : 0.0255
[4/5] Train loss : -0.0047, validation loss : 0.0255
[5/5] Train loss : -0.0047, validation loss : 0.0255
2022-12-16 01:50:26,466 [DEBUG] Training from 2018, 2019
2022-12-16 01:50:26,494 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:50:26,494 [DEBUG] [+] Training model...
[1/3] Train loss : 0.0201, validation loss : -0.0108
[2/3] Train loss : 0.0201, validation loss : -0.0108
[3/3] Train loss : 0.0201, validation loss : -0.0108
2022-12-16 01:52:33,279 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:52:33,281 [DEBUG] [+] Training model...
[1/5] Train loss : 0.0104, validation loss : -0.0402
[2/5] Train loss : 0.0104, validation loss : -0.0402
[3/5] Train loss : 0.0104, validation loss : -0.0402
[4/5] Train loss : 0.0104, validation loss : -0.0402
[5/5] Train loss : 0.0104, validation loss : -0.0402
2022-12-16 01:54:21,318 [DEBUG] Training from 2020, 2021
2022-12-16 01:54:21,341 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:54:21,341 [DEBUG] [+] Training model...
[1/3] Train loss : 0.0758, validation loss : 0.0579
[2/3] Train loss : 0.0758, validation loss : 0.0579
[3/3] Train loss : 0.0758, validation loss : 0.0579
2022-12-16 01:56:13,497 [DEBUG] [+] Setting sgd optimizer
2022-12-16 01:56:13,499 [DEBUG] [+] Training model...
[1/5] Train loss : 0.0291, validation loss : 0.0241
[2/5] Train loss : 0.0291, validation loss : 0.0241
[3/5] Train loss : 0.0291, validation loss : 0.0241
[4/5] Train loss : 0.0291, validation loss : 0.0241
[5/5] Train loss : 0.0291, validation loss : 0.0241
2022-12-16 01:57:39,987 [DEBUG] Backtesting strategies
2022-12-16 01:57:39,997 [DEBUG] First year: 2008
2022-12-16 01:57:40,028 [DEBUG] First year: 2008
2022-12-16 01:57:51,356 [DEBUG] Changing year: 2009
2022-12-16 01:57:51,386 [DEBUG] Changing year: 2009
2022-12-16 01:58:05,589 [DEBUG] Changing year: 2010
2022-12-16 01:58:05,590 [DEBUG] Changing model: lstm - 0
2022-12-16 01:58:05,622 [DEBUG] Changing year: 2010
2022-12-16 01:58:05,623 [DEBUG] Changing model: transformer - 0
2022-12-16 01:58:19,969 [DEBUG] Changing year: 2011
2022-12-16 01:58:20,000 [DEBUG] Changing year: 2011
2022-12-16 01:58:34,514 [DEBUG] Changing year: 2012
2022-12-16 01:58:34,514 [DEBUG] Changing model: lstm - 1
2022-12-16 01:58:34,550 [DEBUG] Changing year: 2012
2022-12-16 01:58:34,551 [DEBUG] Changing model: transformer - 1
2022-12-16 01:58:48,993 [DEBUG] Changing year: 2013
2022-12-16 01:58:49,031 [DEBUG] Changing year: 2013
2022-12-16 01:59:03,618 [DEBUG] Changing year: 2014
2022-12-16 01:59:03,619 [DEBUG] Changing model: lstm - 2
2022-12-16 01:59:03,651 [DEBUG] Changing year: 2014
2022-12-16 01:59:03,653 [DEBUG] Changing model: transformer - 2
2022-12-16 01:59:18,370 [DEBUG] Changing year: 2015
2022-12-16 01:59:18,401 [DEBUG] Changing year: 2015
2022-12-16 01:59:33,263 [DEBUG] Changing year: 2016
2022-12-16 01:59:33,263 [DEBUG] Changing model: lstm - 3
2022-12-16 01:59:33,298 [DEBUG] Changing year: 2016
2022-12-16 01:59:33,300 [DEBUG] Changing model: transformer - 3
2022-12-16 01:59:48,181 [DEBUG] Changing year: 2017
2022-12-16 01:59:48,215 [DEBUG] Changing year: 2017
2022-12-16 02:00:03,067 [DEBUG] Changing year: 2018
2022-12-16 02:00:03,067 [DEBUG] Changing model: lstm - 4
2022-12-16 02:00:03,098 [DEBUG] Changing year: 2018
2022-12-16 02:00:03,100 [DEBUG] Changing model: transformer - 4
2022-12-16 02:00:17,907 [DEBUG] Changing year: 2019
2022-12-16 02:00:17,948 [DEBUG] Changing year: 2019
2022-12-16 02:00:32,579 [DEBUG] Changing year: 2020
2022-12-16 02:00:32,579 [DEBUG] Changing model: lstm - 5
2022-12-16 02:00:32,612 [DEBUG] Changing year: 2020
2022-12-16 02:00:32,614 [DEBUG] Changing model: transformer - 5
2022-12-16 02:00:47,771 [DEBUG] Changing year: 2021
2022-12-16 02:00:47,812 [DEBUG] Changing year: 2021
2022-12-16 02:01:04,687 [DEBUG] Changing year: 2022
2022-12-16 02:01:04,687 [DEBUG] Changing model: lstm - 6
2022-12-16 02:01:04,722 [DEBUG] Changing year: 2022
2022-12-16 02:01:04,724 [DEBUG] Changing model: transformer - 6
2022-12-16 02:01:17,063 [DEBUG] Backtesting results
2022-12-16 02:01:17,064 [DEBUG] ---------------------------------
2022-12-16 02:01:17,064 [DEBUG] Results of strategy: random
2022-12-16 02:01:17,065 [DEBUG] Expected returns: 23.372766843783822%
2022-12-16 02:01:17,066 [DEBUG] Volatilty: 37.6133021461872%
2022-12-16 02:01:17,066 [DEBUG] Sharpe Ratio: 0.6213963015781926
2022-12-16 02:01:17,066 [DEBUG] MDD: -1.5222690998256223
2022-12-16 02:01:17,067 [DEBUG] ---------------------------------
2022-12-16 02:01:17,067 [DEBUG] Results of strategy: equal strategy
2022-12-16 02:01:17,068 [DEBUG] Expected returns: 19.099810829319914%
2022-12-16 02:01:17,068 [DEBUG] Volatilty: 27.671778722561047%
2022-12-16 02:01:17,068 [DEBUG] Sharpe Ratio: 0.6902270728895237
2022-12-16 02:01:17,069 [DEBUG] MDD: -1.2121061373370512
2022-12-16 02:01:17,069 [DEBUG] ---------------------------------
2022-12-16 02:01:17,069 [DEBUG] Results of strategy: lstm
2022-12-16 02:01:17,071 [DEBUG] Expected returns: 17.83542411563072%
2022-12-16 02:01:17,071 [DEBUG] Volatilty: 15.711628373454195%
2022-12-16 02:01:17,071 [DEBUG] Sharpe Ratio: 1.1351734964508715
2022-12-16 02:01:17,072 [DEBUG] MDD: -1.253696272979567
2022-12-16 02:01:17,072 [DEBUG] ---------------------------------
2022-12-16 02:01:17,072 [DEBUG] Results of strategy: transformer
2022-12-16 02:01:17,074 [DEBUG] Expected returns: 14.367503821574942%
2022-12-16 02:01:17,074 [DEBUG] Volatilty: 18.83172932411449%
2022-12-16 02:01:17,074 [DEBUG] Sharpe Ratio: 0.762941287775255
2022-12-16 02:01:17,075 [DEBUG] MDD: -1.2077016479066138
2022-12-16 02:01:17,114 [DEBUG] Saving results
2022-12-16 02:01:17,424 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:01:17,424 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:01:17,424 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:01:17,424 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:01:17,424 [DEBUG] STREAM b'IDAT' 78 1189
2022-12-16 02:01:17,429 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:01:17,429 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:01:17,429 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:01:17,429 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:01:17,430 [DEBUG] STREAM b'IDAT' 78 2994
2022-12-16 02:01:17,485 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:01:17,486 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:01:17,486 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:01:17,486 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:01:17,486 [DEBUG] STREAM b'IDAT' 78 374
2022-12-16 02:01:17,487 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:01:17,488 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:01:17,488 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:01:17,488 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:01:17,488 [DEBUG] STREAM b'IDAT' 78 286
2022-12-16 02:01:17,490 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:01:17,490 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:01:17,490 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:01:17,490 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:01:17,490 [DEBUG] STREAM b'IDAT' 78 263
2022-12-16 02:01:17,492 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:01:17,492 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:01:17,492 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:01:17,492 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:01:17,493 [DEBUG] STREAM b'IDAT' 78 387
2022-12-16 02:01:17,494 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:01:17,494 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:01:17,494 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:01:17,494 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:01:17,494 [DEBUG] STREAM b'IDAT' 78 436
2022-12-16 02:01:17,496 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:01:17,496 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:01:17,496 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:01:17,496 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:01:17,496 [DEBUG] STREAM b'IDAT' 78 351
2022-12-16 02:01:17,498 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:01:17,498 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:01:17,498 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:01:17,498 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:01:17,498 [DEBUG] STREAM b'IDAT' 78 364
2022-12-16 02:01:17,991 [DEBUG] Saving models
