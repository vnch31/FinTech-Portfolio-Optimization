2022-12-16 02:16:01,123 [DEBUG] use config:config.json
2022-12-16 02:16:01,124 [DEBUG] Creating, training and testing models with the following parameters:
2022-12-16 02:16:01,124 [DEBUG] VTI AGG DBC ^VIX
from 2006-01-01 to 2022-10-01 with 1d interval
retrain every 2 year with 30 days

2022-12-16 02:16:01,124 [DEBUG] Getting data
2022-12-16 02:16:01,124 [DEBUG] Fetch data using the following parameters:
2022-12-16 02:16:01,124 [DEBUG] Tickers: VTI AGG DBC ^VIX
2022-12-16 02:16:01,124 [DEBUG] Start date: 2006-01-01
2022-12-16 02:16:01,124 [DEBUG] End date: 2022-10-01
2022-12-16 02:16:01,124 [DEBUG] Interval: 1d
2022-12-16 02:16:01,124 [DEBUG] File dataset_2006-01-01_2022-10-01_VTI_AGG_DBC_^VIX.csv, no need to download, loading file...
2022-12-16 02:16:01,145 [DEBUG]        Unnamed: 0        Date Ticker  ...         Low        Open      Volume
0               0  2006-01-03    AGG  ...   59.801179   59.866725    170600.0
1               1  2006-01-03    VTI  ...   44.781733   45.091552   1769800.0
2               2  2006-01-03   ^VIX  ...   10.990000   12.250000         0.0
3               3  2006-01-04    AGG  ...   59.950149   59.997819    284500.0
4               4  2006-01-04    VTI  ...   45.652858   45.678373    763600.0
...           ...         ...    ...  ...         ...         ...         ...
16836       16836  2022-09-29   ^VIX  ...   31.160000   31.670000         0.0
16837       16837  2022-09-30    AGG  ...   95.320501   95.984443  14376300.0
16838       16838  2022-09-30    DBC  ...   23.840000   24.020000   3947600.0
16839       16839  2022-09-30    VTI  ...  179.279999  181.559998   8431700.0
16840       16840  2022-09-30   ^VIX  ...   29.389999   31.610001         0.0

[16841 rows x 8 columns]
2022-12-16 02:16:01,177 [DEBUG] Training and testing with the device: cuda
2022-12-16 02:16:01,177 [DEBUG] Training model: supertest
2022-12-16 02:16:02,581 [DEBUG] Training from 2006, 2007
2022-12-16 02:16:02,609 [DEBUG] [+] Creating LSTM model
2022-12-16 02:16:02,610 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:16:02,610 [DEBUG] [+] Training model...
[1/3] Train loss : 0.0315, validation loss : 0.0319
[2/3] Train loss : 0.0628, validation loss : 0.0307
[3/3] Train loss : 0.0629, validation loss : 0.0307
2022-12-16 02:16:21,229 [DEBUG] [+] Creating Transformer Encoder model
2022-12-16 02:16:21,245 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:16:21,246 [DEBUG] [+] Training model...
[1/5] Train loss : 0.0562, validation loss : 0.0307
[2/5] Train loss : 0.0629, validation loss : 0.0307
[3/5] Train loss : 0.0629, validation loss : 0.0307
[4/5] Train loss : 0.0629, validation loss : 0.0307
[5/5] Train loss : 0.0629, validation loss : 0.0307
2022-12-16 02:17:37,553 [DEBUG] Training from 2008, 2009
2022-12-16 02:17:37,574 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:17:37,575 [DEBUG] [+] Training model...
[1/3] Train loss : -0.0585, validation loss : 0.0870
[2/3] Train loss : -0.0585, validation loss : 0.0870
[3/3] Train loss : -0.0585, validation loss : 0.0870
2022-12-16 02:18:02,799 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:18:02,800 [DEBUG] [+] Training model...
[1/5] Train loss : -0.0584, validation loss : 0.0870
[2/5] Train loss : -0.0585, validation loss : 0.0870
[3/5] Train loss : -0.0585, validation loss : 0.0870
[4/5] Train loss : -0.0585, validation loss : 0.0870
[5/5] Train loss : -0.0585, validation loss : 0.0870
2022-12-16 02:19:23,323 [DEBUG] Training from 2010, 2011
2022-12-16 02:19:23,344 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:19:23,345 [DEBUG] [+] Training model...
[1/3] Train loss : 0.0438, validation loss : -0.0724
[2/3] Train loss : 0.0438, validation loss : -0.0724
[3/3] Train loss : 0.0438, validation loss : -0.0724
2022-12-16 02:19:46,753 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:19:46,754 [DEBUG] [+] Training model...
[1/5] Train loss : 0.0438, validation loss : -0.0724
[2/5] Train loss : 0.0438, validation loss : -0.0724
[3/5] Train loss : 0.0438, validation loss : -0.0724
[4/5] Train loss : 0.0438, validation loss : -0.0724
[5/5] Train loss : 0.0438, validation loss : -0.0724
2022-12-16 02:21:00,219 [DEBUG] Training from 2012, 2013
2022-12-16 02:21:00,241 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:21:00,241 [DEBUG] [+] Training model...
[1/3] Train loss : 0.0239, validation loss : 0.0771
[2/3] Train loss : 0.0239, validation loss : 0.0771
[3/3] Train loss : 0.0239, validation loss : 0.0771
2022-12-16 02:21:24,652 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:21:24,653 [DEBUG] [+] Training model...
[1/5] Train loss : 0.0239, validation loss : 0.0771
[2/5] Train loss : 0.0239, validation loss : 0.0771
[3/5] Train loss : 0.0239, validation loss : 0.0771
[4/5] Train loss : 0.0239, validation loss : 0.0771
[5/5] Train loss : 0.0239, validation loss : 0.0771
2022-12-16 02:22:51,402 [DEBUG] Training from 2014, 2015
2022-12-16 02:22:51,418 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:22:51,418 [DEBUG] [+] Training model...
[1/3] Train loss : -0.0007, validation loss : -0.0698
[2/3] Train loss : -0.0007, validation loss : -0.0698
[3/3] Train loss : -0.0007, validation loss : -0.0698
2022-12-16 02:23:14,960 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:23:14,961 [DEBUG] [+] Training model...
[1/5] Train loss : -0.0007, validation loss : -0.0698
[2/5] Train loss : -0.0007, validation loss : -0.0698
[3/5] Train loss : -0.0007, validation loss : -0.0698
[4/5] Train loss : -0.0007, validation loss : -0.0698
[5/5] Train loss : -0.0007, validation loss : -0.0698
2022-12-16 02:24:33,192 [DEBUG] Training from 2016, 2017
2022-12-16 02:24:33,211 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:24:33,212 [DEBUG] [+] Training model...
[1/3] Train loss : 0.0119, validation loss : 0.0641
[2/3] Train loss : 0.0119, validation loss : 0.0641
[3/3] Train loss : 0.0119, validation loss : 0.0641
2022-12-16 02:24:58,415 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:24:58,416 [DEBUG] [+] Training model...
[1/5] Train loss : 0.0119, validation loss : 0.0641
[2/5] Train loss : 0.0119, validation loss : 0.0641
[3/5] Train loss : 0.0119, validation loss : 0.0641
[4/5] Train loss : 0.0119, validation loss : 0.0641
[5/5] Train loss : 0.0119, validation loss : 0.0641
2022-12-16 02:26:24,725 [DEBUG] Training from 2018, 2019
2022-12-16 02:26:24,742 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:26:24,742 [DEBUG] [+] Training model...
[1/3] Train loss : 0.0143, validation loss : 0.0005
[2/3] Train loss : 0.0143, validation loss : 0.0005
[3/3] Train loss : 0.0143, validation loss : 0.0005
2022-12-16 02:26:48,652 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:26:48,653 [DEBUG] [+] Training model...
[1/5] Train loss : 0.0143, validation loss : 0.0005
[2/5] Train loss : 0.0143, validation loss : 0.0005
[3/5] Train loss : 0.0143, validation loss : 0.0005
[4/5] Train loss : 0.0143, validation loss : 0.0005
[5/5] Train loss : 0.0143, validation loss : 0.0005
2022-12-16 02:28:23,279 [DEBUG] Training from 2020, 2021
2022-12-16 02:28:23,297 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:28:23,298 [DEBUG] [+] Training model...
[1/3] Train loss : 0.0769, validation loss : 0.0691
[2/3] Train loss : 0.0769, validation loss : 0.0691
[3/3] Train loss : 0.0769, validation loss : 0.0691
2022-12-16 02:28:47,843 [DEBUG] [+] Setting sgd optimizer
2022-12-16 02:28:47,845 [DEBUG] [+] Training model...
[1/5] Train loss : 0.0769, validation loss : 0.0691
[2/5] Train loss : 0.0769, validation loss : 0.0691
[3/5] Train loss : 0.0769, validation loss : 0.0691
[4/5] Train loss : 0.0769, validation loss : 0.0691
[5/5] Train loss : 0.0769, validation loss : 0.0691
2022-12-16 02:30:12,779 [DEBUG] Backtesting strategies
2022-12-16 02:30:12,796 [DEBUG] First year: 2008
2022-12-16 02:30:12,811 [DEBUG] First year: 2008
2022-12-16 02:30:20,618 [DEBUG] Changing year: 2009
2022-12-16 02:30:20,626 [DEBUG] Changing year: 2009
2022-12-16 02:30:29,689 [DEBUG] Changing year: 2010
2022-12-16 02:30:29,690 [DEBUG] Changing model: lstm - 0
2022-12-16 02:30:29,705 [DEBUG] Changing year: 2010
2022-12-16 02:30:29,706 [DEBUG] Changing model: transformer - 0
2022-12-16 02:30:38,879 [DEBUG] Changing year: 2011
2022-12-16 02:30:38,889 [DEBUG] Changing year: 2011
2022-12-16 02:30:48,554 [DEBUG] Changing year: 2012
2022-12-16 02:30:48,554 [DEBUG] Changing model: lstm - 1
2022-12-16 02:30:48,569 [DEBUG] Changing year: 2012
2022-12-16 02:30:48,570 [DEBUG] Changing model: transformer - 1
2022-12-16 02:30:57,535 [DEBUG] Changing year: 2013
2022-12-16 02:30:57,546 [DEBUG] Changing year: 2013
2022-12-16 02:31:06,848 [DEBUG] Changing year: 2014
2022-12-16 02:31:06,849 [DEBUG] Changing model: lstm - 2
2022-12-16 02:31:06,859 [DEBUG] Changing year: 2014
2022-12-16 02:31:06,859 [DEBUG] Changing model: transformer - 2
2022-12-16 02:31:16,314 [DEBUG] Changing year: 2015
2022-12-16 02:31:16,331 [DEBUG] Changing year: 2015
2022-12-16 02:31:24,999 [DEBUG] Changing year: 2016
2022-12-16 02:31:24,999 [DEBUG] Changing model: lstm - 3
2022-12-16 02:31:25,014 [DEBUG] Changing year: 2016
2022-12-16 02:31:25,014 [DEBUG] Changing model: transformer - 3
2022-12-16 02:31:33,465 [DEBUG] Changing year: 2017
2022-12-16 02:31:33,474 [DEBUG] Changing year: 2017
2022-12-16 02:31:41,810 [DEBUG] Changing year: 2018
2022-12-16 02:31:41,811 [DEBUG] Changing model: lstm - 4
2022-12-16 02:31:41,822 [DEBUG] Changing year: 2018
2022-12-16 02:31:41,822 [DEBUG] Changing model: transformer - 4
2022-12-16 02:31:50,492 [DEBUG] Changing year: 2019
2022-12-16 02:31:50,502 [DEBUG] Changing year: 2019
2022-12-16 02:31:59,760 [DEBUG] Changing year: 2020
2022-12-16 02:31:59,760 [DEBUG] Changing model: lstm - 5
2022-12-16 02:31:59,769 [DEBUG] Changing year: 2020
2022-12-16 02:31:59,770 [DEBUG] Changing model: transformer - 5
2022-12-16 02:32:09,450 [DEBUG] Changing year: 2021
2022-12-16 02:32:09,461 [DEBUG] Changing year: 2021
2022-12-16 02:32:21,702 [DEBUG] Changing year: 2022
2022-12-16 02:32:21,702 [DEBUG] Changing model: lstm - 6
2022-12-16 02:32:21,713 [DEBUG] Changing year: 2022
2022-12-16 02:32:21,713 [DEBUG] Changing model: transformer - 6
2022-12-16 02:32:29,197 [DEBUG] Backtesting results
2022-12-16 02:32:29,197 [DEBUG] ---------------------------------
2022-12-16 02:32:29,198 [DEBUG] Results of strategy: random
2022-12-16 02:32:29,200 [DEBUG] Expected returns: 13.868516273167836%
2022-12-16 02:32:29,200 [DEBUG] Volatilty: 37.88949087498198%
2022-12-16 02:32:29,201 [DEBUG] Sharpe Ratio: 0.3660254058025643
2022-12-16 02:32:29,202 [DEBUG] MDD: -1.4775867970230512
2022-12-16 02:32:29,202 [DEBUG] ---------------------------------
2022-12-16 02:32:29,202 [DEBUG] Results of strategy: equal strategy
2022-12-16 02:32:29,203 [DEBUG] Expected returns: 19.5730706714877%
2022-12-16 02:32:29,203 [DEBUG] Volatilty: 27.640171995858577%
2022-12-16 02:32:29,204 [DEBUG] Sharpe Ratio: 0.7081385265772008
2022-12-16 02:32:29,204 [DEBUG] MDD: -1.2121061373370512
2022-12-16 02:32:29,204 [DEBUG] ---------------------------------
2022-12-16 02:32:29,204 [DEBUG] Results of strategy: lstm
2022-12-16 02:32:29,205 [DEBUG] Expected returns: 18.031060634980864%
2022-12-16 02:32:29,205 [DEBUG] Volatilty: 15.685239339557183%
2022-12-16 02:32:29,205 [DEBUG] Sharpe Ratio: 1.1495559770967387
2022-12-16 02:32:29,206 [DEBUG] MDD: -1.2536963368380774
2022-12-16 02:32:29,206 [DEBUG] ---------------------------------
2022-12-16 02:32:29,206 [DEBUG] Results of strategy: transformer
2022-12-16 02:32:29,207 [DEBUG] Expected returns: 18.031021790936386%
2022-12-16 02:32:29,207 [DEBUG] Volatilty: 15.685241693939911%
2022-12-16 02:32:29,208 [DEBUG] Sharpe Ratio: 1.1495533280754469
2022-12-16 02:32:29,208 [DEBUG] MDD: -1.2536963009590165
2022-12-16 02:32:29,258 [DEBUG] Saving results
2022-12-16 02:32:29,614 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:32:29,614 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:32:29,614 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:32:29,614 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:32:29,614 [DEBUG] STREAM b'IDAT' 78 1189
2022-12-16 02:32:29,618 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:32:29,618 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:32:29,618 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:32:29,618 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:32:29,619 [DEBUG] STREAM b'IDAT' 78 2994
2022-12-16 02:32:29,681 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:32:29,681 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:32:29,681 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:32:29,681 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:32:29,682 [DEBUG] STREAM b'IDAT' 78 374
2022-12-16 02:32:29,684 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:32:29,684 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:32:29,684 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:32:29,685 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:32:29,685 [DEBUG] STREAM b'IDAT' 78 286
2022-12-16 02:32:29,687 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:32:29,688 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:32:29,688 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:32:29,688 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:32:29,688 [DEBUG] STREAM b'IDAT' 78 263
2022-12-16 02:32:29,691 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:32:29,691 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:32:29,692 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:32:29,692 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:32:29,692 [DEBUG] STREAM b'IDAT' 78 387
2022-12-16 02:32:29,694 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:32:29,695 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:32:29,695 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:32:29,695 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:32:29,695 [DEBUG] STREAM b'IDAT' 78 436
2022-12-16 02:32:29,697 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:32:29,697 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:32:29,698 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:32:29,698 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:32:29,698 [DEBUG] STREAM b'IDAT' 78 351
2022-12-16 02:32:29,699 [DEBUG] STREAM b'IHDR' 16 13
2022-12-16 02:32:29,699 [DEBUG] STREAM b'sBIT' 41 4
2022-12-16 02:32:29,699 [DEBUG] b'sBIT' 41 4 (unknown)
2022-12-16 02:32:29,700 [DEBUG] STREAM b'pHYs' 57 9
2022-12-16 02:32:29,700 [DEBUG] STREAM b'IDAT' 78 364
2022-12-16 02:32:30,465 [DEBUG] Saving models
